# Student Performance Predictor #

If the data is split 30/70 for testing/training data, the models achieve
mediocre performance ranging from 63% to 66%.

If the data is split 50/50, the models achieve more optimal performance
nearing 70-72%.

When training the models with all the data, and testing with 50%, the model
accuracy is just barely lower than before.

When doing the same but testing against 30%, the model performance decreases
to 68%.

* When splitting data 50/50 and having an accuracy of 71.7%, the confusion
  matrix is:
       0   1
       _____
  0 |  26 47 
    |
  1 |  9 116

  The model is not precise for not passing, but is very good for passing
  predictions.

  It is implied that this model sees a correlation in pass rates and not fail
  rates. Can all students that are not predicted to pass be considered failing?

  One interesting fix is to lower the pass the threshold from 10 to 6, which
  increases the true negative rate but reduces the true positive rate by a
  significant margin. The accuracy of this approach is 47%.

* It seems we can reduce the false positive rate by adjusting how we train the
  model. Instead of using a one-shot approach to train the model, use online
  training to repeatedly push the classification in a certain direction. In
  this case by repeatedly making the model biased towards 0, the false
  positives are reduced, and our accuracy is improved to be 73.23% when
  testing against half the data and 73.16% when testing against all the data.

  Further modifications reveal that we can achieve 72.66% with a better false
  positive rate.
